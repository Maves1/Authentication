{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-18T17:45:27.285679Z",
     "start_time": "2024-04-18T17:45:27.282113Z"
    }
   },
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from hmog import HmogHelper\n",
    "from tqdm import tqdm\n",
    "\n",
    "dataset_path = 'preprocessed_data/'"
   ],
   "outputs": [],
   "execution_count": 51
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-18T17:45:27.560539Z",
     "start_time": "2024-04-18T17:45:27.558170Z"
    }
   },
   "source": [
    "WINDOW_SIZE = 40000  # ms\n",
    "WINDOW_SIZES = [20000, 40000, 60000, 80000, 100000, 120000, 140000]"
   ],
   "outputs": [],
   "execution_count": 52
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-18T17:54:35.731551Z",
     "start_time": "2024-04-18T17:54:35.728736Z"
    }
   },
   "source": [
    "class SimpleNeuralNet(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super(SimpleNeuralNet, self).__init__()\n",
    "        \n",
    "        input_shape = 64\n",
    "        \n",
    "        # self.conv1 = nn.Conv1d(in_channels=input_shape, out_channels=64, kernel_size=5, stride=1, padding=2)\n",
    "        self.fc1 = nn.Linear(input_shape, 32)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(in_features=32, out_features=16)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(in_features=16, out_features=1)\n",
    "        self.out = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.out(x)\n",
    "        \n",
    "        return x"
   ],
   "outputs": [],
   "execution_count": 73
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-18T17:54:36.280507Z",
     "start_time": "2024-04-18T17:54:36.276657Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, X, y, transform=None):\n",
    "        # self.dataset = np.load(file_path)\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.transform = transform\n",
    "        \n",
    "        # self.X = self.dataset[:, 1:-1]\n",
    "        # self.y = self.dataset[:, -1]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # if self.transform is not None:\n",
    "        #     curr_item_x = self.transform(self.X[idx])\n",
    "        #     curr_item_y = self.transform(self.y[idx])\n",
    "        #     \n",
    "        #     return curr_item_x, curr_item_y\n",
    "    \n",
    "        return self.X[idx], self.y[idx]"
   ],
   "outputs": [],
   "execution_count": 74
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-18T17:54:36.636294Z",
     "start_time": "2024-04-18T17:54:36.432806Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "whole_dataset = np.load(os.path.join(dataset_path, 'hmog_vectors_with_labels.npy'))\n",
    "whole_dataset = whole_dataset.astype(np.float32)\n",
    "X, y = whole_dataset[:, 1:-1], whole_dataset[:, -1]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "train_dataset = MyDataset(X_train, y_train, transform=None)\n",
    "test_dataset = MyDataset(X_test, y_test, transform=None)\n",
    "\n",
    "print(len(train_dataset), len(test_dataset))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "417497 104375\n"
     ]
    }
   ],
   "execution_count": 75
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-18T17:54:36.640120Z",
     "start_time": "2024-04-18T17:54:36.637342Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train_one_epoch(dataloader, loss_fn, device):\n",
    "    running_loss = 0.\n",
    "    last_loss = 0.\n",
    "\n",
    "    for i, data in enumerate(dataloader):\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device, dtype=torch.float32), labels.to(device, dtype=torch.float32)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs).reshape(-1)\n",
    "\n",
    "        # Compute the loss and its gradients\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "\n",
    "        # Adjust learning weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # Gather data and report\n",
    "        running_loss += loss.item()\n",
    "        if i % 1000 == 999:\n",
    "            last_loss = running_loss / 1000 # loss per batch\n",
    "            print('  batch {} loss: {}'.format(i + 1, last_loss))\n",
    "            running_loss = 0.\n",
    "\n",
    "    return last_loss"
   ],
   "outputs": [],
   "execution_count": 76
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-18T17:55:32.152311Z",
     "start_time": "2024-04-18T17:54:36.723355Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = SimpleNeuralNet()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'mps')\n",
    "model.to(device)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "EPOCHS = 3\n",
    "\n",
    "for epoch in tqdm(range(EPOCHS)):\n",
    "    avg_loss = train_one_epoch(train_dataloader, criterion, device)\n",
    "\n",
    "    running_vloss = 0.0\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    # Disable gradient computation and reduce memory consumption.\n",
    "    with torch.no_grad():\n",
    "        for i, vdata in enumerate(test_dataloader):\n",
    "            vinputs, vlabels = vdata\n",
    "            vinputs, vlabels = vinputs.to(device, dtype=torch.float32), vlabels.to(device, dtype=torch.float32)\n",
    "            \n",
    "            voutputs = model(vinputs).reshape(-1)\n",
    "            vloss = criterion(voutputs, vlabels)\n",
    "            running_vloss += vloss\n",
    "\n",
    "    avg_vloss = running_vloss / (i + 1)\n",
    "    print('LOSS train {} valid {}'.format(avg_loss, avg_vloss))"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 1000 loss: 43.052040670394895\n",
      "  batch 2000 loss: 41.63147640037537\n",
      "  batch 3000 loss: 40.07008311080933\n",
      "  batch 4000 loss: 38.42133229255676\n",
      "  batch 5000 loss: 36.66354320335388\n",
      "  batch 6000 loss: 36.94552133846283\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 1/3 [00:18<00:36, 18.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS train 36.94552133846283 valid 34.85208511352539\n",
      "  batch 1000 loss: 33.93632242488861\n",
      "  batch 2000 loss: 14.750022595822811\n",
      "  batch 3000 loss: 1.1236298621296883\n",
      "  batch 4000 loss: 0.712294006884098\n",
      "  batch 5000 loss: 0.6909599542617798\n",
      "  batch 6000 loss: 0.6909055152535438\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 2/3 [00:36<00:18, 18.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS train 0.6909055152535438 valid 0.6465810537338257\n",
      "  batch 1000 loss: 0.6360754384100438\n",
      "  batch 2000 loss: 0.6276267566680909\n",
      "  batch 3000 loss: 0.6271793736815453\n",
      "  batch 4000 loss: 0.668477309256792\n",
      "  batch 5000 loss: 0.6753668496310711\n",
      "  batch 6000 loss: 0.61218160623312\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:55<00:00, 18.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS train 0.61218160623312 valid 0.6540545225143433\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 77
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-18T18:13:50.459142Z",
     "start_time": "2024-04-18T18:13:50.447455Z"
    }
   },
   "cell_type": "code",
   "source": "  torch.save(model, 'classification_model')",
   "outputs": [],
   "execution_count": 79
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
